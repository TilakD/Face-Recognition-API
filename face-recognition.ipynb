{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "plot_model(nn4_small2_pretrained, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='model_plot.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Facenet model\n",
    "\n",
    "The CNN architecture used here is a variant of the inception architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_model\n",
    "\n",
    "nn4_small2 = create_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model training aims to learn an embedding $f(x)$ of image $x$ such that the squared L2 distance between all faces of the same identity is small and the distance between a pair of faces from different identities is large. This can be achieved with a *triplet loss* $L$ that is minimized when the distance between an anchor image $x^a_i$ and a positive image $x^p_i$ (same identity) in embedding space is smaller than the distance between that anchor image and a negative image $x^n_i$ (different identity) by at least a margin $\\alpha$.\n",
    "\n",
    "$$L = \\sum^{m}_{i=1} \\large[ \\small {\\mid \\mid f(x_{i}^{a}) - f(x_{i}^{p})) \\mid \\mid_2^2} - {\\mid \\mid f(x_{i}^{a}) - f(x_{i}^{n})) \\mid \\mid_2^2} + \\alpha \\large ] \\small_+$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "s='jpg'\n",
    "a=\"jpeg\"    \n",
    "b=\"JPG\" \n",
    "c=\"png\" \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        #print(i)\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "             if f.endswith(s) or f.endswith(a)  or f.endswith(b) or f.endswith(c):\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata = load_metadata('data')\n",
    "print(metadata)\n",
    "print(metadata.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from align import AlignDlib\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    #destRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img[...,::-1]\n",
    "    #return destRGB\n",
    "\n",
    "# Initialize the OpenFace face alignment utility\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "# Load an image \n",
    "jc_orig = load_image(metadata[55].image_path())\n",
    "\n",
    "# Detect face and return bounding box\n",
    "bb = alignment.getLargestFaceBoundingBox(jc_orig)\n",
    "\n",
    "# Transform image using specified face landmark indices and crop image to 96x96\n",
    "jc_aligned = alignment.align(96, jc_orig, bb, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "\n",
    "# Show original image\n",
    "f = plt.figure(figsize=(20,20))\n",
    "f.add_subplot(131)\n",
    "\n",
    "#plt.subplots(figsize=(4, 4))\n",
    "plt.imshow(jc_orig)\n",
    "#f.savefig(\"example.jpg\")\n",
    "# Show original image with bounding box\n",
    "f.add_subplot(132)\n",
    "#subplots(figsize=(4, 4))\n",
    "plt.imshow(jc_orig)\n",
    "plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "#f.savefig(\"detect_face.jpg\")\n",
    "# Show aligned image\n",
    "f.add_subplot(133)\n",
    "#plt.subplots(figsize=(4, 4))\n",
    "plt.imshow(jc_aligned);\n",
    "#f.savefig(\"align_face.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_image(img):\n",
    "    return alignment.align(96, img, alignment.getLargestFaceBoundingBox(img), \n",
    "                           landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding vectors can now be calculated by feeding the aligned and scaled images into the pre-trained network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "embedded = np.zeros((metadata.shape[0], 128))\n",
    "\n",
    "#print(metadata)\n",
    "k=0\n",
    "for i, m in enumerate(metadata):\n",
    "    \n",
    "        print(i,m.image_path())\n",
    "        img = load_image(m.image_path())\n",
    "        \n",
    "        img = align_image(img)\n",
    "        # scale RGB values to interval [0,1]\n",
    "        if img is not None:\n",
    "            img = (img / 255.).astype(np.float32)\n",
    "        # obtain embedding vector for image\n",
    "            embedded[i] = nn4_small2_pretrained.predict(np.expand_dims(img, axis=0))[0]\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify on a single triplet example that the squared L2 distance between its anchor-positive pair is smaller than the distance between its anchor-negative pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(emb1, emb2):\n",
    "    return np.sum(np.square(emb1 - emb2))\n",
    "\n",
    "def show_pair(idx1, idx2):\n",
    "    f=plt.figure(figsize=(10,5))\n",
    "    plt.suptitle(f'Distance = {distance(embedded[idx1], embedded[idx2]):.2f}')\n",
    "    plt.subplot(121)\n",
    "    \n",
    "    plt.imshow(load_image(metadata[idx1].image_path()))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(load_image(metadata[idx2].image_path())); \n",
    " \n",
    "show_pair(0, 8)\n",
    "show_pair(0, 126)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "distances = [] # squared L2 distance between pairs\n",
    "identical = [] # 1 if same identity, 0 otherwise\n",
    "\n",
    "num = len(metadata)\n",
    "\n",
    "for i in range(num - 1):\n",
    "    for j in range(1, num):\n",
    "        distances.append(distance(embedded[i], embedded[j]))\n",
    "        identical.append(1 if metadata[i].name == metadata[j].name else 0)\n",
    "        \n",
    "distances = np.array(distances)\n",
    "identical = np.array(identical)\n",
    "\n",
    "thresholds = np.arange(0.3, 1.0, 0.01)\n",
    "\n",
    "f1_scores = [f1_score(identical, distances < t) for t in thresholds]\n",
    "acc_scores = [accuracy_score(identical, distances < t) for t in thresholds]\n",
    "\n",
    "opt_idx = np.argmax(f1_scores)\n",
    "# Threshold at maximal F1 score\n",
    "opt_tau = thresholds[opt_idx]\n",
    "# Accuracy at maximal F1 score\n",
    "opt_acc = accuracy_score(identical, distances < opt_tau)\n",
    "\n",
    "# Plot F1 score and accuracy as function of distance threshold\n",
    "plt.plot(thresholds, f1_scores, label='F1 score');\n",
    "plt.plot(thresholds, acc_scores, label='Accuracy');\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title(f'Accuracy at threshold {opt_tau:.2f} = {opt_acc:.3f}');\n",
    "plt.xlabel('Distance threshold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_pos = distances[identical == 1]\n",
    "dist_neg = distances[identical == 0]\n",
    "\n",
    "fig=plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(dist_pos)\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title('Distances (pos. pairs)')\n",
    "plt.legend();\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.hist(dist_neg)\n",
    "plt.axvline(x=opt_tau, linestyle='--', lw=1, c='lightgrey', label='Threshold')\n",
    "plt.title('Distances (neg. pairs)')\n",
    "plt.legend();\n",
    "fig.savefig(\"dist_histo.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "targets = np.array([m.name for m in metadata])\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) % 2 != 0\n",
    "test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "\n",
    "# 50 train examples of 10 identities (5 examples each)\n",
    "X_train = embedded[train_idx]\n",
    "# 50 test examples of 10 identities (5 examples each)\n",
    "X_test = embedded[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "y_test = y[test_idx]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1,metric='euclidean')\n",
    "svc = SVC(kernel='linear', probability=True)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "acc_knn = accuracy_score(y_test, knn.predict(X_test))\n",
    "acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training with all  traininig data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "targets = np.array([m.name for m in metadata])\n",
    "\n",
    "#print(targets)\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(targets)\n",
    "\n",
    "# Numerical encoding of identities\n",
    "y = encoder.transform(targets)\n",
    "\n",
    "train_idx = np.arange(metadata.shape[0]) \n",
    "\n",
    "#test_idx = np.arange(metadata.shape[0]) % 2 == 0\n",
    "\n",
    "\n",
    "# 50 train examples of 10 identities (5 examples each)\n",
    "X_train = embedded[train_idx]\n",
    "# 50 test examples of 10 identities (5 examples each)\n",
    "#X_test = embedded[test_idx]\n",
    "\n",
    "y_train = y[train_idx]\n",
    "#y_test = y[test_idx]\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1, metric='euclidean')\n",
    "svc = SVC(kernel='linear', probability=True)\n",
    "\n",
    "knn.fit(X_train, y_train)\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "#acc_knn = accuracy_score(y_test, knn.predict(X_test))\n",
    "#acc_svc = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "#print(f'KNN accuracy = {acc_knn}, SVM accuracy = {acc_svc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "knn_model_save_path = \"saved_models//knn_model.sav\"\n",
    "if knn_model_save_path != \"\":\n",
    "    with open(knn_model_save_path, 'wb') as f:\n",
    "        pickle.dump(knn, f)\n",
    "        \n",
    "svc_model_save_path = \"saved_models//svc_model.sav\"\n",
    "if svc_model_save_path != \"\":\n",
    "    with open(svc_model_save_path, 'wb') as f:\n",
    "        pickle.dump(svc, f)\n",
    "        \n",
    "label_model_save_path = \"saved_models//label_model.sav\"\n",
    "if label_model_save_path != \"\":\n",
    "    with open(label_model_save_path, 'wb') as f:\n",
    "        pickle.dump(encoder, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed the dataset into 2D space for displaying identity clusters, [t-distributed Stochastic Neighbor Embedding](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) (t-SNE) is applied to the 128-dimensional embedding vectors. Except from a few outliers, identity clusters are well separated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embedded)\n",
    "#print(X_embedded)\n",
    "fig=plt.figure(figsize=(15,8))\n",
    "for i, t in enumerate(set(targets)):\n",
    "    idx = targets == t\n",
    "    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1], label=t)   \n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1));\n",
    "fig.savefig(\"cluster.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "myFile = open('embedding.csv', 'w')  \n",
    "with myFile:  \n",
    "   writer = csv.writer(myFile)\n",
    "   writer.writerows(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save model as pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_model\n",
    "import tensorflow as tf\n",
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_session(session, keep_var_names=None, output_names=None, clear_devices=True):\n",
    "    \"\"\"\n",
    "    Freezes the state of a session into a pruned computation graph.\n",
    "\n",
    "    Creates a new computation graph where variable nodes are replaced by\n",
    "    constants taking their current value in the session. The new graph will be\n",
    "    pruned so subgraphs that are not necessary to compute the requested\n",
    "    outputs are removed.\n",
    "    @param session The TensorFlow session to be frozen.\n",
    "    @param keep_var_names A list of variable names that should not be frozen,\n",
    "                          or None to freeze all the variables in the graph.\n",
    "    @param output_names Names of the relevant graph outputs.\n",
    "    @param clear_devices Remove the device directives from the graph for better portability.\n",
    "    @return The frozen graph definition.\n",
    "    \"\"\"\n",
    "    graph = session.graph\n",
    "    with graph.as_default():\n",
    "        freeze_var_names = list(set(v.op.name for v in tf.global_variables()).difference(keep_var_names or []))\n",
    "        output_names = output_names or []\n",
    "        output_names += [v.op.name for v in tf.global_variables()]\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        if clear_devices:\n",
    "            for node in input_graph_def.node:\n",
    "                node.device = \"\"\n",
    "        frozen_graph = tf.graph_util.convert_variables_to_constants(\n",
    "            session, input_graph_def, output_names, freeze_var_names)\n",
    "        return frozen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "frozen_graph = freeze_session(K.get_session(),\n",
    "                              output_names=[out.op.name for out in nn4_small2_pretrained.outputs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.write_graph(frozen_graph, \"pb_file\", \"my_model.pb\", as_text=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model  -  Independent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "knn_model_save_path = \"saved_models//knn_model.sav\"\n",
    "svc_model_save_path = \"saved_models//svc_model.sav\"\n",
    "label_model_save_path = \"saved_models//label_model.sav\"\n",
    "with open(knn_model_save_path, 'rb') as f:\n",
    "    knn = pickle.load(f)\n",
    "with open(svc_model_save_path, 'rb') as f:\n",
    "    svc = pickle.load(f)\n",
    "with open(label_model_save_path, 'rb') as f:\n",
    "    encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import create_model\n",
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from align import AlignDlib\n",
    "import numpy as np\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "camera = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    return_value,image = camera.read()\n",
    "    frame = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imshow('image',image)\n",
    "    if cv2.waitKey(1)& 0xFF == ord('r'):\n",
    "        bb = alignment.getAllFaceBoundingBoxes(frame)\n",
    "        im_aligned=[]\n",
    "        for i in bb:\n",
    "              j = alignment.align(96, frame, i, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "              im_aligned.append(j)   \n",
    "        for n in im_aligned:\n",
    "            n = (n / 255.).astype(np.float32)\n",
    "            embedded_t = nn4_small2_pretrained.predict(np.expand_dims(n, axis=0))[0]\n",
    "            #print(embedded_t)\n",
    "            embedded_t=embedded_t.reshape(1,-1)\n",
    "            #print( embedded_t)\n",
    "            example_predic = knn.predict(embedded_t)\n",
    "            example_prob = svc.predict_proba(embedded_t)\n",
    "            print(f\"Probablities of each class is {example_prob}\")\n",
    "            if np.any(example_prob>0.15):\n",
    "                example_i = encoder.inverse_transform(example_predic)[0]\n",
    "                print(f'Recognized as {example_i}')\n",
    "            else:\n",
    "                print(\"Not a face from database...\")\n",
    "    if cv2.waitKey(1)& 0xFF == ord('q'):\n",
    "        break\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from align import AlignDlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "# Initialize the OpenFace face alignment utility\n",
    "\n",
    "start_time = time.clock()\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    #destRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img[...,::-1]\n",
    "    #return destRGB\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    alignment = AlignDlib('models//landmarks.dat')\n",
    "\n",
    "    # Load an image of Jacques Chirac\n",
    "    im_orig = load_image('test_images//t6.jpg')\n",
    "\n",
    "    # Detect face and return bounding box\n",
    "    bb = alignment.getAllFaceBoundingBoxes(im_orig)\n",
    "    #print(bb)\n",
    "    # Transform image using specified face landmark indices and crop image to 96x96\n",
    "    im_aligned=[]\n",
    "    for i in bb:\n",
    "          j = alignment.align(96, im_orig, i, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "          im_aligned.append(j)   \n",
    "\n",
    "    image_height_tensor = tf.placeholder(tf.int32)\n",
    "    image_width_tensor = tf.placeholder(tf.int32)\n",
    "\n",
    "    # Show original image\n",
    "    #plt.subplot(221)\n",
    "    #plt.imshow(im_orig)\n",
    "    export_path = \"inference_graph//V11\"\n",
    "    builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
    "    fig=plt.figure(figsize=(15,15))\n",
    "    plt.imshow(im_orig)\n",
    "    # Show original image with bounding box\n",
    "    #plt.subplot(222)\n",
    "    #print(im_aligned)\n",
    "    #plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "    for i in bb:\n",
    "        plt.gca().add_patch(patches.Rectangle((i.left(), i.top()), i.width(), i.height(), fill=False, color='red',linewidth=2))\n",
    "\n",
    "    # Show aligned image\n",
    "    #plt.subplot(133)\n",
    "    #fig.savefig('time_hog_detection.jpg')\n",
    "    #print(\"--- %s seconds ---\" % (time.clock() - start_time))\n",
    "    #for n in im_aligned:\n",
    "    n = (n / 255.).astype(np.float32)\n",
    "    print(\"n is\")\n",
    "    #print(n.shape)\n",
    "    new_n = np.expand_dims(n, axis=0)\n",
    "    #print(\"new_n is\")\n",
    "   # print(new_n.shape)\n",
    "    tensor_info_input = tf.saved_model.utils.build_tensor_info(tf.convert_to_tensor(n))\n",
    "    tensor_info_height = tf.saved_model.utils.build_tensor_info(image_height_tensor)\n",
    "    tensor_info_width = tf.saved_model.utils.build_tensor_info(image_width_tensor)\n",
    "\n",
    "    embedded_t = nn4_small2_pretrained.predict(new_n)[0]\n",
    "    print(embedded_t.shape)\n",
    "\n",
    "    #print(embedded)\n",
    "    embedded_t=embedded_t.reshape(1,-1)\n",
    "    print(embedded_t.shape)\n",
    "    print( embedded_t)\n",
    "    tensor_info_output = tf.saved_model.utils.build_tensor_info(tf.convert_to_tensor(embedded_t))\n",
    "    example_predic = knn.predict(embedded_t)\n",
    "    example_prob = svc.predict_proba(embedded_t)\n",
    "    #print(example_prob)\n",
    "    if np.any(example_prob>0.50):\n",
    "        example_i = encoder.inverse_transform(example_predic)[0]\n",
    "        plt.subplots(figsize=(4, 4))\n",
    "        plt.imshow(n)\n",
    "\n",
    "        plt.title(f'Recognized as {example_i}')\n",
    "\n",
    "    else:\n",
    "            plt.subplots(figsize=(4, 4))\n",
    "            plt.imshow(n)\n",
    "\n",
    "            plt.title('N/A')\n",
    "    prediction_signature = (\n",
    "            tf.saved_model.signature_def_utils.build_signature_def(\n",
    "                inputs={'images': tensor_info_input, 'height': tensor_info_height, 'width': tensor_info_width},\n",
    "                outputs={'segmentation_map': tensor_info_output},\n",
    "                method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME))\n",
    "    builder.add_meta_graph_and_variables(\n",
    "            sess, [tf.saved_model.tag_constants.SERVING],\n",
    "            signature_def_map={\n",
    "                'predict_images':\n",
    "                    prediction_signature,\n",
    "            })\n",
    "\n",
    "    # export the model\n",
    "    builder.save(as_text=True)\n",
    "    print('Done exporting!')\n",
    "    print(\"--- %s seconds ---\" % (time.clock() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def printTensors(pb_file):\n",
    "\n",
    "    # read pb into graph_def\n",
    "    with tf.gfile.GFile(pb_file, \"rb\") as f:\n",
    "        graph_def = tf.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "    # import graph_def\n",
    "    with tf.Graph().as_default() as graph:\n",
    "        tf.import_graph_def(graph_def)\n",
    "\n",
    "    # print operations\n",
    "    for op in graph.get_operations():\n",
    "        print(op.name)\n",
    "\n",
    "\n",
    "printTensors(\"pb_file/my_model.pb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.saved_model import signature_constants\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "export_dir = 'inference_graph/V26'\n",
    "graph_pb = 'pb_file/my_model.pb'\n",
    "\n",
    "builder = tf.saved_model.builder.SavedModelBuilder(export_dir)\n",
    "\n",
    "with tf.gfile.GFile(graph_pb, \"rb\") as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "sigs = {}\n",
    "\n",
    "with tf.Session(graph=tf.Graph()) as sess:\n",
    "    # name=\"\" is important to ensure we don't get spurious prefixing\n",
    "    tf.import_graph_def(graph_def, name=\"\")\n",
    "    g = tf.get_default_graph()\n",
    "    inp = g.get_tensor_by_name(\"conv1/kernel:0\")\n",
    "    out = g.get_tensor_by_name(\"norm_layer_2/l2_normalize/Sum/reduction_indices:0\")\n",
    "\n",
    "    sigs[signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY] = \\\n",
    "        tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "            {\"in\": inp}, {\"out\": out})\n",
    "\n",
    "    builder.add_meta_graph_and_variables(sess,\n",
    "                                         [tag_constants.SERVING],\n",
    "                                         signature_def_map=sigs)\n",
    "\n",
    "builder.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correct pbtxt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.applications import VGG19\n",
    "from keras.models import Model\n",
    "from model import create_model\n",
    "# very important to do this as a first thing\n",
    "K.set_learning_phase(0)\n",
    "nn4_small2_pretrained = create_model()\n",
    "nn4_small2_pretrained.load_weights('weights/nn4.small2.v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import utils\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import build_signature_def, predict_signature_def, signature_constants\n",
    "from tensorflow.contrib.session_bundle import exporter\n",
    "\n",
    "export_path = 'inference_graph/V11'\n",
    "builder = saved_model_builder.SavedModelBuilder(export_path)\n",
    "prediction_signature = (tf.saved_model.signature_def_utils.build_signature_def(\n",
    "            inputs={'images':tf.saved_model.utils.build_tensor_info(nn4_small2_pretrained.input)},\n",
    "            outputs={'scores':tf.saved_model.utils.build_tensor_info(nn4_small2_pretrained.output)},\n",
    "            method_name=tf.saved_model.signature_constants.PREDICT_METHOD_NAME)) \n",
    "\n",
    "#signature = predict_signature_def(inputs={'images': nn4_small2_pretrained.input},outputs={'scores': nn4_small2_pretrained.output})\n",
    "\n",
    "with K.get_session() as sess:\n",
    "    builder.add_meta_graph_and_variables(sess=sess,\n",
    "                                         tags=[tag_constants.SERVING],\n",
    "                                         signature_def_map={'predict': prediction_signature})\n",
    "    builder.save(as_text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization and Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import genfromtxt\n",
    "import numpy as np\n",
    "embedded = genfromtxt('embedding.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "class IdentityMetadata():\n",
    "    def __init__(self, base, name, file):\n",
    "        # dataset base directory\n",
    "        self.base = base\n",
    "        # identity name\n",
    "        self.name = name\n",
    "        # image file name\n",
    "        self.file = file\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.image_path()\n",
    "\n",
    "    def image_path(self):\n",
    "        return os.path.join(self.base, self.name, self.file) \n",
    "s='jpg'\n",
    "a=\"jpeg\"    \n",
    "b=\"JPG\" \n",
    "c=\"png\" \n",
    "def load_metadata(path):\n",
    "    metadata = []\n",
    "    for i in os.listdir(path):\n",
    "        #print(i)\n",
    "        for f in os.listdir(os.path.join(path, i)):\n",
    "             if f.endswith(s) or f.endswith(a)  or f.endswith(b) or f.endswith(c):\n",
    "                metadata.append(IdentityMetadata(path, i, f))\n",
    "    return np.array(metadata)\n",
    "\n",
    "metadata = load_metadata('data')\n",
    "#print(metadata)\n",
    "print(metadata.shape[0])\n",
    "targets = np.array([m.name for m in metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "X_embedded = TSNE(n_components=2).fit_transform(embedded)\n",
    "#print(X_embedded)\n",
    "fig=plt.figure(figsize=(15,8))\n",
    "for i, t in enumerate(set(targets)):\n",
    "    idx = targets == t\n",
    "    plt.scatter(X_embedded[idx, 0], X_embedded[idx, 1],label=t)   \n",
    "    \n",
    "plt.legend(bbox_to_anchor=(1, 1));\n",
    "fig.savefig(\"cluster.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "inception_layers = [ layer.name for layer in nn4_small2_pretrained.layers]\n",
    "print(inception_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "activation_9_extractor = Model(inputs=nn4_small2_pretrained.input, outputs=nn4_small2_pretrained.get_layer('activation_39').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_9_featres = activation_9_extractor.predict(np.expand_dims(n, axis=0))\n",
    "print(activation_9_featres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(activation_9_featres[0, :, :, 0], cmap='jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_maps(feature_maps):\n",
    "    height, width, depth = feature_maps.shape\n",
    "    nb_plot = int(np.rint(np.sqrt(depth)))\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    for i in range(depth):\n",
    "        plt.subplot(nb_plot, nb_plot, i+1)\n",
    "        plt.imshow(feature_maps[:,:,i], cmap='jet')\n",
    "        plt.title('feature map {}'.format(i+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_layer(layer_name):    \n",
    "    features_extractor = Model(inputs=nn4_small2_pretrained.input, outputs=nn4_small2_pretrained.get_layer(layer_name).output)\n",
    "    feature_maps = features_extractor.predict(np.expand_dims(n, axis=0))[0]\n",
    "    print(\"At layer \\\"{}\\\" : {} \".format(layer_name, feature_maps.shape))\n",
    "    plot_feature_maps(feature_maps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_layer('conv1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_layer('conv2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_layer('conv3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_layer('concatenate_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_layer('concatenate_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_layer('concatenate_3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer('concatenate_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer('concatenate_5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_layer('concatenate_6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_layer('concatenate_7')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DUMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from align import AlignDlib\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Initialize the OpenFace face alignment utility\n",
    "\n",
    "start_time = time.clock()\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    #destRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img[...,::-1]\n",
    "    #return destRGB\n",
    "\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "\n",
    "\n",
    "vc = cv2.VideoCapture(0)\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "if vc.isOpened(): # try to get the first frame\n",
    "    is_capturing, frame = vc.read()\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    # makes the blues image look real colored\n",
    "    webcam_preview = plt.imshow(frame)    \n",
    "else:\n",
    "    is_capturing = False\n",
    "\n",
    "while is_capturing:\n",
    "    try:    # Lookout for a keyboardInterrupt to stop the script\n",
    "        is_capturing, frame = vc.read()\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    # makes the blues image look real colored\n",
    "        \n",
    "        ##Detection here\n",
    "        # Detect face and return bounding box\n",
    "        bb = alignment.getAllFaceBoundingBoxes(frame)\n",
    "        im_aligned=[]\n",
    "        for i in bb:\n",
    "              j = alignment.align(96, frame, i, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "              im_aligned.append(j)   \n",
    "        for i in bb:\n",
    "            plt.gca().add_patch(patches.Rectangle((i.left(), i.top()), i.width(), i.height(), fill=False, color='red',linewidth=2))\n",
    "        for n in im_aligned:\n",
    "            n = (n / 255.).astype(np.float32)\n",
    "            embedded_t = nn4_small2_pretrained.predict(np.expand_dims(n, axis=0))[0]\n",
    "            #print(embedded)\n",
    "            embedded_t=embedded_t.reshape(1,-1)\n",
    "            #print( embedded_t)\n",
    "            example_predic = knn.predict(embedded_t)\n",
    "            example_prob = svc.predict_proba(embedded_t)\n",
    "            #print(example_prob)\n",
    "            if np.any(example_prob>0.25):\n",
    "                example_i = encoder.inverse_transform(example_predic)[0]\n",
    "                print(f'Recognized as {example_i}')\n",
    "           \n",
    "            else:\n",
    "                print('N/A')\n",
    "        webcam_preview.set_data(frame)\n",
    "        plt.draw()\n",
    "        try:    # Avoids a NotImplementedError caused by `plt.pause`\n",
    "            plt.pause(0.05)\n",
    "        except Exception:\n",
    "            pass\n",
    "    except KeyboardInterrupt:\n",
    "        vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Try one\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import time\n",
    "from align import AlignDlib\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "# Initialize the OpenFace face alignment utility\n",
    "\n",
    "start_time = time.clock()\n",
    "def load_image(path):\n",
    "    img = cv2.imread(path, 1)\n",
    "    # OpenCV loads images with color channels\n",
    "    # in BGR order. So we need to reverse them\n",
    "    #destRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    return img[...,::-1]\n",
    "    #return destRGB\n",
    "\n",
    "alignment = AlignDlib('models/landmarks.dat')\n",
    "\n",
    "# Load an image of Jacques Chirac\n",
    "im_orig = load_image('test_images//t6.jpg')\n",
    "\n",
    "# Detect face and return bounding box\n",
    "bb = alignment.getAllFaceBoundingBoxes(im_orig)\n",
    "#print(bb)\n",
    "# Transform image using specified face landmark indices and crop image to 96x96\n",
    "im_aligned=[]\n",
    "for i in bb:\n",
    "      j = alignment.align(96, im_orig, i, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "      im_aligned.append(j)   \n",
    "\n",
    "# Show original image\n",
    "#plt.subplot(221)\n",
    "#plt.imshow(im_orig)\n",
    "\n",
    "fig=plt.figure(figsize=(15,15))\n",
    "plt.imshow(im_orig)\n",
    "# Show original image with bounding box\n",
    "#plt.subplot(222)\n",
    "#print(im_aligned)\n",
    "#plt.gca().add_patch(patches.Rectangle((bb.left(), bb.top()), bb.width(), bb.height(), fill=False, color='red'))\n",
    "for i in bb:\n",
    "    \n",
    "    plt.gca().add_patch(patches.Rectangle((i.left(), i.top()), i.width(), i.height(), fill=False, color='red',linewidth=2))\n",
    "\n",
    "# Show aligned image\n",
    "#plt.subplot(133)\n",
    "#fig.savefig('time_hog_detection.jpg')\n",
    "#print(\"--- %s seconds ---\" % (time.clock() - start_time))\n",
    "for n in im_aligned:\n",
    "    n = (n / 255.).astype(np.float32)\n",
    "    embedded_t = nn4_small2_pretrained.predict(np.expand_dims(n, axis=0))[0]\n",
    "    print(embedded_t)\n",
    "    print(\"Reshaping\")\n",
    "    embedded_t=embedded_t.reshape(1,-1)\n",
    "    print( embedded_t)\n",
    "\n",
    "    example_predic = knn.predict(embedded_t)\n",
    "    example_prob = svc.predict_proba(embedded_t)\n",
    "    #print(example_prob)\n",
    "    if np.any(example_prob>0.25):\n",
    "        example_i = encoder.inverse_transform(example_predic)[0]\n",
    "        plt.subplots(figsize=(4, 4))\n",
    "        plt.imshow(n)\n",
    "    \n",
    "        plt.title(f'Recognized as {example_i}')\n",
    "           \n",
    "    else:\n",
    "            plt.subplots(figsize=(4, 4))\n",
    "            plt.imshow(n)\n",
    "\n",
    "            plt.title('N/A')\n",
    "print(\"--- %s seconds ---\" % (time.clock() - start_time))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import cv2\n",
    "from PIL import Image, ImageTk\n",
    "\n",
    "width, height = 800, 600\n",
    "cap = cv2.VideoCapture(0)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "\n",
    "root = tk.Tk()\n",
    "root.bind('<Escape>', lambda e: root.quit())\n",
    "lmain = tk.Label(root)\n",
    "lmain.pack()\n",
    "\n",
    "def show_frame():\n",
    "    _, frame = cap.read()\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    cv2image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGBA)\n",
    "    bb = alignment.getAllFaceBoundingBoxes(cv2image)\n",
    "    im_aligned=[]\n",
    "    for i in bb:\n",
    "          j = alignment.align(96, frame, i, landmarkIndices=AlignDlib.OUTER_EYES_AND_NOSE)\n",
    "          im_aligned.append(j)   \n",
    "    for i in bb:\n",
    "        plt.gca().add_patch(patches.Rectangle((i.left(), i.top()), i.width(), i.height(), fill=False, color='red',linewidth=2))\n",
    "    for n in im_aligned:\n",
    "        n = (n / 255.).astype(np.float32)\n",
    "        embedded_t = nn4_small2_pretrained.predict(np.expand_dims(n, axis=0))[0]\n",
    "        #print(embedded)\n",
    "        embedded_t=embedded_t.reshape(1,-1)\n",
    "        #print( embedded_t)\n",
    "        example_predic = knn.predict(embedded_t)\n",
    "        example_prob = svc.predict_proba(embedded_t)\n",
    "        #print(example_prob)\n",
    "        if np.any(example_prob>0.45):\n",
    "            example_i = encoder.inverse_transform(example_predic)[0]\n",
    "            print(f'Recognized as {example_i}')\n",
    "\n",
    "        else:\n",
    "            print('N/A')\n",
    "    img = Image.fromarray(cv2image)\n",
    "    imgtk = ImageTk.PhotoImage(image=img)\n",
    "    lmain.imgtk = imgtk\n",
    "    lmain.configure(image=imgtk)\n",
    "    lmain.after(10, show_frame)\n",
    "\n",
    "show_frame()\n",
    "root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
